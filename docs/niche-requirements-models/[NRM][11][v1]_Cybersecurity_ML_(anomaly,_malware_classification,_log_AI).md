# **Cybersecurity ML (anomaly, malware classification, log AI) — Niche Requirements Model (v1)**

## **1\) Workload anatomy (the “physics”)**

### **Typical job types you must support**

* **Dev/Test**: parse a sample of logs/pcaps/binaries, validate features, run small training/inference  
* **Log ingestion \+ normalization**: parse many sources, schema mapping, dedup, enrichment  
* **Feature extraction**:  
  * logs → sequences/features (time windows)  
  * binaries → static features (strings/opcodes/graphs)  
  * network → flow features (pcap → flows)  
* **Batch training**:  
  * anomaly models, classifiers, sequence models for logs  
  * malware family classification  
* **Batch inference / scoring**: score huge log volumes, triage alerts  
* **Streaming detection**: near-realtime anomaly detection and alerting  
* **LLM “log AI”**: summarization/explanations over alert context (careful governance)  
* **Graph analytics** (optional): entity graphs (user/device/IP) \+ GNN-like models  
* **Eval/regression**: precision/recall, alert volume, drift checks, replay attacks

### **Workload profile (p50 / p90)**

| Job type | Typical scale | GPU config (p50 → p90) | CPU/RAM (p50 → p90) | Local NVMe | Runtime (p50 → p90) | I/O pattern |
| ----- | ----- | ----- | ----- | ----- | ----- | ----- |
| Dev/Test (parsing \+ small runs) | 1–50GB logs / small corpus | 0 GPUs → 1×24GB | 16–32 vCPU, 64–256GB | 0.5–2TB | 30 min–4h → 1–3 days | Lots of parsing, reruns, debug logs |
| Log ingestion \+ normalization | 10GB–10TB/day | 0 GPUs → 1×24GB | 32–256 vCPU, 128GB–2TB | 1–16TB | continuous / hourly batches | Heavy read/write; schema mapping; enrichment joins |
| Feature extraction (pcap/flows) | 1–100TB/day (org-dependent) | 0 GPUs → 1×24GB | 32–512 vCPU, 128GB–4TB | 1–32TB | near-realtime → days backlog | Very I/O heavy; CPU-bound; intermediate artifacts |
| Malware feature extraction (static) | 10k–10M files/day | 0 GPUs → 1×24GB | 32–256 vCPU, 128GB–2TB | 1–16TB | minutes–hours → days | Many small files; sandboxing optional; metadata writes |
| Batch training (tabular/log models) | 10M–10B events | 0–1×24GB → 4×48GB | 32–256 vCPU, 128GB–2TB | 1–16TB | 2–12h → 1–14 days | Streaming reads; checkpoints; metric logs |
| Batch training (deep seq/graph) | large corpora/graphs | 1×48GB → 8×80GB | 64–512 vCPU, 256GB–4TB | 2–32TB | 1–7 days → 2–8 weeks | High throughput reads; big checkpoints; DDP scaling |
| Batch inference / scoring | 10GB–10TB/day | 0–1×24GB → 2×48GB | 32–256 vCPU, 128GB–2TB | 1–16TB | 0.1×–1× realtime → 1×–5× realtime | Read-heavy \+ writes to alert store; batching key |
| Streaming detection (prod) | 10k–10M events/s (varies) | 0–1×24GB → 2×48GB | 32–512 vCPU, 128GB–4TB | 200GB–2TB | continuous | Tail latency sensitive; stateful windows; backpressure needed |
| LLM “log AI” summarization | per alert/case | 1×24GB → 2×48GB | 16–128 vCPU, 64GB–512GB | 200GB–2TB | seconds → minutes | Reads context; writes short text; governance \+ redaction |

### **Parallelism expectations**

* Most of the work is **CPU \+ I/O bound** (parsing, enrichment, flow extraction).  
* GPU matters mainly for:  
  * deep sequence models / transformer encoders  
  * LLM-assisted summarization (if self-hosted)  
* Scale pattern: shard by time range / source / tenant.

### **Failure tolerance**

* Pipelines must be restartable/idempotent:  
  * by `source_id`, `time_window`, `shard_id`  
* Streaming needs:  
  * durable offsets (Kafka-like)  
  * replay capability  
* Spot/preemptible:  
  * good for batch features/training if resumable  
  * not for latency-critical streaming unless you have redundancy

**Output artifact:** workload profile with p50/p90 resource \+ time numbers (table above), refined later via telemetry.

---

## **2\) Stack \+ environment constraints**

### **Required frameworks / tooling (typical)**

* **Data plumbing**: Kafka-like streams, object storage lakes, SIEM integrations  
* **Parsing**: lots of vendor formats; normalization to a common schema  
* **ML**: sklearn/xgboost for classic; PyTorch for deep seq/graph; optional ONNX/TensorRT  
* **Search/index**: fast retrieval of context (alerts/cases), sometimes vector search for “similar incidents”  
* **Sandboxing (malware)**: if offered, requires strict isolation and often nested virtualization or hardened containers (many providers avoid this initially)

### **CUDA/driver constraints**

* Only needed for GPU model serving/training; stable CUDA 12.x baseline is fine.  
* Pin model/runtime versions for reproducibility and incident RCA.

### **Container vs VM requirements**

* Containers are default.  
* VMs/dedicated tenancy often required for sensitive customers and for malware handling/sandboxing.

### **Storage expectations**

* Local NVMe for:  
  * hot shards, intermediate artifacts, feature caches  
* Durable storage for:  
  * raw logs (often immutable), derived features, models, alerts  
* Retention can be huge (compliance), so lifecycle policies are key.

### **Networking needs**

* Usually requires private ingress from customer networks (VPN/peering)  
* Egress often locked down; allowlists only  
* High ingest throughput for big orgs

**Output artifact:** reference runtime image set

* `sec-ingest-normalize`  
* `sec-feature-extract`  
* `sec-train-cpu`  
* `sec-train-cu12`  
* `sec-score-batch`  
* `sec-detect-stream`  
* `sec-llm-log-ai-cu12`  
* `compat`

---

## **3\) Data governance \+ risk**

### **Data sensitivity**

* Logs can contain PII, secrets, internal IPs, infrastructure details.  
* Malware samples are dangerous.

### **Common requirements**

* Strong isolation (often dedicated projects/nodes)  
* Encryption at rest \+ TLS in transit  
* Strict RBAC \+ audit logs  
* Data residency / region lock  
* Retention policies \+ legal hold support

### **Logging rules (important irony)**

* Your platform processes logs, but your platform logs must not leak sensitive customer data.  
* Default: redact secrets/tokens, minimize payload logging.

**Output artifact:** security/compliance checklist \+ tenancy requirements.

---

## **4\) Operational expectations (what they’ll blame you for)**

### **What matters most**

* Detection latency (time-to-alert)  
* Precision/recall tradeoffs (false positives kill trust)  
* Pipeline reliability (no dropped events)  
* Reproducibility (why did this alert fire?)  
* Cost control (log volumes are massive)

### **SLO starting points (priors)**

* Streaming ingestion durability: “no data loss” for accepted events  
* Detection latency p95 targets per customer (varies)  
* Batch completion reliability: \>99% with resume/retry  
* Observability: backlog size, offsets, throughput, alert rates, error budgets

### **Support playbook themes**

* “Alert storm” (thresholds, model drift)  
* “We missed incidents” (coverage gaps, schema changes)  
* “Pipeline lagging” (backpressure, partitioning, I/O saturation)  
* “Data access denied” (private networking/IAM)  
* “Security review” (audit evidence, isolation proof)

### **Pricing sensitivity & billing preference**

* Usually priced by:  
  * ingest volume (GB/day) \+ retention  
  * compute for feature extraction/training  
  * detection/alert tiers  
* Enterprises want predictable monthly bills; reservations common.

**Output artifact:** SLOs \+ support playbook \+ pricing notes.

---

## **5\) User workflow & integrations**

### **How they launch work today**

* Connect log sources → normalize → store → detect → triage → respond  
* Offline: periodic model retrains \+ replay over historical data  
* “Log AI”: analyst opens a case and wants summarization \+ recommended next steps

### **Auth patterns**

* Org/team RBAC \+ service accounts  
* SOC teams need tight access controls \+ audit

### **Artifact flow**

* Inputs: logs/flows/binaries \+ enrichment feeds (asset inventory, IAM)  
* Outputs: features, models, alerts, case summaries, run manifests  
* Destinations: SIEM/SOAR systems \+ customer storage

### **Golden flows (end-to-end)**

1. **Streaming anomaly detection**  
   * Ingest → normalize → windowed features → score → alert → case context store → dashboards  
2. **Malware classification pipeline**  
   * Upload/ingest samples → static feature extract → classify → store verdict \+ explainers → export to SOC tooling  
3. **Analyst “log AI” case summary**  
   * Pull related alerts \+ entity graph → redact sensitive fields → generate concise summary → store in case system with audit trail

